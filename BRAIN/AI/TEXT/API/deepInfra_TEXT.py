import json
import requests
from typing import Union
import os
import re
from dotenv import load_dotenv; load_dotenv() # Load environment variables from .env file

def generate(message: str, model: str='meta-llama/Meta-Llama-3-70B-Instruct', system_prompt: str = "Be Helpful and Friendly. Keep your response straightfoward, short and concise", max_tokens: int = 512, temperature: float = 0.7, stream: bool = False, chunk_size: int = 24) -> Union[str, None]:
    """
    Utilizes a variety of large language models (LLMs) to engage in conversational interactions.
    
    Parameters:
        - model (str): The name or identifier of the LLM to be used for conversation. Available models include:
            - "meta-llama/Meta-Llama-3-70B-Instruct"
            - "meta-llama/Meta-Llama-3-8B-Instruct" 
            - "mistralai/Mixtral-8x22B-Instruct-v0.1"
            - "mistralai/Mixtral-8x22B-v0.1"
            - "microsoft/WizardLM-2-8x22B"
            - "microsoft/WizardLM-2-7B"
            - "HuggingFaceH4/zephyr-orpo-141b-A35b-v0.1"
            - "google/gemma-1.1-7b-it"
            - "databricks/dbrx-instruct"
            - "mistralai/Mixtral-8x7B-Instruct-v0.1"
            - "mistralai/Mistral-7B-Instruct-v0.2"
            - "meta-llama/Llama-2-70b-chat-hf"
            - "cognitivecomputations/dolphin-2.6-mixtral-8x7b"
        - message (str): The message to be sent to the LLM to initiate or continue the conversation.
        - system_prompt (str): Optional. The initial system message to start the conversation. Defaults to "Talk Like Shakespeare".
        - max_tokens (int): Optional. The maximum number of tokens to be generated by the LLM. Defaults to 512.
        - temperature (float): Optional. The temperature of the LLM. Defaults to 0.7.
        - stream (bool): Optional. Whether to stream the response from the LLM. Defaults to True.
        - chunk_size (int): Optional. The size of the chunks to be streamed from the LLM. Defaults to 24.

    Returns:
        - Union[str, None]: The response message from the LLM if successful, otherwise None.
    """
    api_url = "https://api.deepinfra.com/v1/openai/chat/completions"
    headers ={
        "Authorization" : f"Bearer {os.environ.get('DEEPINFRA')}"
    }
    payload = {
        "model": model,
        "messages": [
            {"role": "system", "content": system_prompt}
        ] + [
            {"role": "user", "content": message}
        ],
        "temperature": temperature,
        "max_tokens": max_tokens,
        "stop": [],
        "stream": True
    }

    
    try:
        response = requests.post(api_url, headers=headers, json=payload, stream=True)
        streaming_text = ""
        for value in response.iter_lines(decode_unicode=True, chunk_size=chunk_size):
            modified_value = re.sub("data:", "", value)
            if modified_value and "[DONE]" not in modified_value:
                json_modified_value = json.loads(modified_value)
                try:
                    if json_modified_value["choices"][0]["delta"]["content"] != None:
                        if stream: print(json_modified_value["choices"][0]["delta"]["content"], end="")
                        streaming_text += json_modified_value["choices"][0]["delta"]["content"]
                except: continue
        return streaming_text
    
    except Exception as e:
        print("Error:", e)
        return "Response content: " + response.text
    

if __name__ == "__main__":

    model_names = [
        "meta-llama/Meta-Llama-3-70B-Instruct",
        "meta-llama/Meta-Llama-3-8B-Instruct",
        "mistralai/Mixtral-8x22B-Instruct-v0.1",
        "mistralai/Mixtral-8x22B-v0.1",
        "microsoft/WizardLM-2-8x22B",
        "microsoft/WizardLM-2-7B",
        "HuggingFaceH4/zephyr-orpo-141b-A35b-v0.1",
        "google/gemma-1.1-7b-it",
        "databricks/dbrx-instruct",
        "mistralai/Mixtral-8x7B-Instruct-v0.1",
        "mistralai/Mistral-7B-Instruct-v0.2",
        "meta-llama/Llama-2-70b-chat-hf",
        "cognitivecomputations/dolphin-2.6-mixtral-8x7b"
    ]

    for name in model_names:
        messages =  "Introduce yourself and tell who made you and about your owner company" # Add more messages as needed
        print(f"\nâ€¢ Model: {name} -")
        response = generate(messages, model=name, system_prompt="Respond very detailed", stream=True)
